[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "benchmark-capture"
version = "0.2.4"
description = "Lightweight profiling decorator for pytest-benchmark"
readme = "README.md"
requires-python = ">=3.9"
license = "Apache-2.0"
license-files = ["LICENSE"]
authors = [
    {name = "benchmark-capture contributors"}
]
keywords = ["benchmark", "profiling", "pytest", "vllm", "neuron", "gpu"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Testing",
    "Topic :: System :: Benchmark",
]

dependencies = []

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-benchmark>=4.0.0",
    "pytest-cov>=4.1.0",
    "black>=23.0.0",
    "isort>=5.12.0",
    "mypy>=1.5.0",
    "flake8>=6.1.0",
]
init = [
    "click>=8.0.0",
    "jinja2>=3.0.0",
]

[project.urls]
Homepage = "https://github.com/littlemex/benchmark-capture"
Repository = "https://github.com/littlemex/benchmark-capture"
Issues = "https://github.com/littlemex/benchmark-capture/issues"

[project.scripts]
benchmark-capture-init = "benchmark_capture.cli_init:init_cmd"

[tool.setuptools.packages.find]
where = ["."]
include = ["benchmark_capture*"]

[tool.setuptools.package-data]
benchmark_capture = ["templates/**/*", "examples/**/*"]

[tool.black]
line-length = 100
target-version = ["py39", "py310", "py311", "py312"]
include = '\.pyi?$'

[tool.isort]
profile = "black"
line_length = 100
multi_line_output = 3
include_trailing_comma = true

[tool.mypy]
python_version = "3.9"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
strict_equality = true

[[tool.mypy.overrides]]
module = "pytest_benchmark.*"
ignore_missing_imports = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "-v",
    "--strict-markers",
    "--cov=benchmark_capture",
    "--cov-report=html",
    "--cov-report=term-missing",
]
# Real-time logging output (especially useful for long-running benchmarks)
log_cli = true
log_cli_level = "INFO"
log_cli_format = "%(asctime)s [%(levelname)s] %(message)s"
log_cli_date_format = "%H:%M:%S"
# File logging for detailed diagnostics
log_file = "pytest.log"
log_file_level = "DEBUG"
log_file_format = "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
log_file_date_format = "%Y-%m-%d %H:%M:%S"

[tool.coverage.run]
source = ["benchmark_capture"]
omit = [
    "tests/*",
    "**/__pycache__/*",
    "benchmark_capture/templates/*",  # Exclude template files (runtime-generated code)
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
    "@abstractmethod",
]
